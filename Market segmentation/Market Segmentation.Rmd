
---
title: "Market segmentation"

output:
  github_document
---




### Removing Spam, Adult, Chatter and Uncategorized as these columns are introducing noise into the Data


```{r echo=FALSE, results='hide',message=FALSE}



library(ggplot2)
library(LICORS)  # for kmeans++
library(foreach)
library(mosaic)

Market_seg = read.csv("/Users/maverick/Documents/MSBA SUMMER SEM/Predictive modelling 2/project/social_marketing.csv", row.names=1)


```


```{r , results='hide',message=FALSE}

Market_seg = subset(Market_seg, select = -c(spam,adult ,chatter, uncategorized))
head(Market_seg)

```



### Running K-means on the Data 


```{r  results='hide',message=FALSE}

set.seed(12)

# Center and scale the data
X = scale(Market_seg, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

# Run k-means with 6 clusters and 25 starts
clust1 = kmeans(X, 6, nstart=25)


```

### Cluster 1 Importance

```{r}
#Sorted values of importance of each cluster

sort(clust1$center[1,]*sigma + mu,decreasing=TRUE)
length(which(clust1$cluster == 1))

```
### Cluster 2 Importance
```{r}

sort(clust1$center[2,]*sigma + mu, decreasing=TRUE)
length(which(clust1$cluster == 2))
```
### Cluster 3 Importance


```{r}


sort(clust1$center[3,]*sigma + mu,decreasing=TRUE)
length(which(clust1$cluster == 3))


```

### Cluster 4 Importance
```{r}


sort(clust1$center[4,]*sigma + mu,decreasing=TRUE)
length(which(clust1$cluster == 4))
```


### Cluster 5 Importance
```{r}

sort(clust1$center[5,]*sigma + mu,decreasing=TRUE)
length(which(clust1$cluster == 5))

```

### Cluster 6 Importance
```{r}

sort(clust1$center[6,]*sigma + mu,decreasing=TRUE)
length(which(clust1$cluster == 6))

```




```{r}

# qplot is in the ggplot2 library
qplot(parenting,family , data=Market_seg, color=factor(clust1$cluster))
```


```{r}
qplot(cooking, photo_sharing, data=Market_seg, color=factor(clust1$cluster))

```


### Running K-Means++

```{r}


X = scale(Market_seg, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")


clust2 = kmeanspp(X, k=6, nstart=25)

```

### Cluster 1 Importance
```{r}
sort(clust2$center[1,]*sigma + mu,decreasing=TRUE)
length(which(clust2$cluster == 1))

```
### Cluster 2 Importance
```{r}
sort(clust2$center[2,]*sigma + mu,decreasing=TRUE)
length(which(clust2$cluster == 2))

```
### Cluster 3 Importance
```{r}
sort(clust2$center[3,]*sigma + mu,decreasing=TRUE)
length(which(clust2$cluster == 3))

```


### Cluster 4 Importance

```{r}
sort(clust2$center[4,]*sigma + mu,decreasing=TRUE)
length(which(clust2$cluster == 4))

```
### Cluster 5 Importance
```{r}
sort(clust2$center[5,]*sigma + mu,decreasing=TRUE)
length(which(clust2$cluster == 5))
```

### Cluster 6 Importance
```{r}
sort(clust2$center[6,]*sigma + mu,decreasing=TRUE)
length(which(clust2$cluster == 6))

```

```{r}


# Compare versus within-cluster average distances from the first run
clust1$withinss
clust2$withinss


sum(clust1$withinss)
sum(clust2$withinss)

clust1$tot.withinss
clust2$tot.withinss

clust1$betweenss
clust2$betweenss



```



#PC Correlation 

```{r echo=FALSE, results='hide',message=FALSE}
# Load data on gene expression in cancer cells


library(corrplot)
library(RColorBrewer)
library(scales)

library(tidyverse)
```

```{r}
M=cor(Market_seg)
corrplot(M, method="color")

#we can see the correlations are quite high and positive hence we can proceed with PCA

```


### We can see the correlation is high so we can run PC to get good results



```{r}


dim(Market_seg)  

pr_Market_seg = prcomp(Market_seg, center = TRUE,scale=TRUE)

summary(pr_Market_seg)
plot(pr_Market_seg, type='l')




```




```{r}





screeplot(pr_Market_seg, type = "l", npcs = 40, main = "Screeplot")
abline(h = 1, col="red", lty=5)
legend("topright", legend=c("Eigenvalue = 1"),
       col=c("red"), lty=5, cex=0.6)

#We see that it is eigen value=1 at 10

cumpro <- cumsum(pr_Market_seg$sdev^2 / sum(pr_Market_seg$sdev^2))
plot(cumpro[0:40], xlab = "PC #", ylab = "Amount of explained variance", main = "Cumulative variance plot")
abline(v =10, col="blue", lty=5)
abline(h = 0.65008, col="blue", lty=5)
legend("topleft", legend=c("Cut-off @ PC10"),
       col=c("blue"), lty=5, cex=0.6)


#changed h= 0.65008 (Cumulative Proportion of PC10 and c=10)







```


```{r}

scores = pr_Market_seg$x




# first, what are PCs themselves?
loadings = pr_Market_seg$rotation



```

Obtain value of Scores[1:10]

```{r}
scores = pr_Market_seg$x

# First for principal components
comp <- data.frame(scores[,1:10])

```



### Running PCA with Kmeans++


```{r}





X = scale(comp, center=TRUE, scale=TRUE)

k <- kmeanspp(X, 6, nstart=25, iter.max=1000)

palette(alpha(brewer.pal(9,'Set1'), 0.5))

mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")


plot(X, col=k$clust, pch=16)

```

### Compare within-cluster average distances of K-means, K-means++ and PCA & K-means++


```{r}


clust1$withinss
clust2$withinss
k$withinss
sum(clust1$withinss)
sum(clust2$withinss)
sum(k$withinss)
clust1$tot.withinss
clust2$tot.withinss
k$tot.withinss
clust1$betweenss
clust2$betweenss
k$betweenss






```

### We see that PC and k-means++ has the least Sum of Squares Value. 

### Hence we can group clusters as the following :


### Cluster 1 Importance
```{r}




sort(k$center[1,]*sigma + mu,decreasing=TRUE)


loadings[,4] %>% sort %>% tail(5)
loadings[,1] %>% sort %>% tail(5)


length(which(k$cluster == 1))
```

### Cluster 2 Importance  

```{r}

sort(k$center[2,]*sigma + mu,decreasing=TRUE)

loadings[,5] %>% sort %>% tail(5)
loadings[,1] %>% sort %>% tail(5)
loadings[,8] %>% sort %>% tail(5)
loadings[,6] %>% sort %>% tail(5)


length(which(k$cluster == 2))


```

### Cluster 3 Importance  


```{r}


sort(k$center[3,]*sigma + mu,decreasing=TRUE)
loadings[,2] %>% sort %>% tail(5)
loadings[,5] %>% sort %>% tail(5)


length(which(k$cluster == 3))

```

### Cluster 4 Importance  

```{r}
sort(k$center[4,]*sigma + mu,decreasing=TRUE)
loadings[,3] %>% sort %>% tail(5)
loadings[,1] %>% sort %>% tail(5)
loadings[,4] %>% sort %>% tail(5)
loadings[,5] %>% sort %>% tail(5)

length(which(k$cluster == 4))

```

### Cluster 5 Importance  


```{r}

sort(k$center[5,]*sigma + mu,decreasing=TRUE)
loadings[,7] %>% sort %>% tail(5)
loadings[,8] %>% sort %>% tail(5)
loadings[,3] %>% sort %>% tail(5)


length(which(k$cluster == 5))
```

### Cluster 6 Importance  


```{r}
sort(k$center[6,]*sigma + mu,decreasing=TRUE)
loadings[,6] %>% sort %>% tail(5)
loadings[,1] %>% sort %>% tail(5)
loadings[,3] %>% sort %>% tail(5)




length(which(k$cluster == 6))
```






### Conclusion 


After running K means on PC, we see that there is high correlation between the six clusters. Here is what we gathered from the results of running a K-means ++ algorithm on the subset of data obtained from PC Analysis.


**Middle aged Parents:**  In the first cluster, we see that News and Politics pops up as well as personal fitness and health nutrition. Some other features include parenting, sports_fandom and school. This led us to believe that this cluster could consist mostly of adults who are probably parents and those who follow the news and politics consistently. They tend to focus on nutritional welfare of their kids and are constantly on the look out for new recipes to experiement for their kids. People in this assumed age bracket also tend to be middle aged and hence more conscious about their health & fitness.


**Stay at home Mums:**  In the second cluster, we see a high importance for beauty, fashion,cooking ,shopping,photo-sharing and art. We can assume that this cluster caters to young women and mothers. People in this cluster also follow the news, watch films on tv and are interested in art. Online gaming is an interesting topic to have surfaced in this cluster as well.

**Working mothers:** In the third cluster, we see a high importance for religion, parenting and food and school. The same can also be observed for shopping, cooking, fashion and beauty. Thus, we believe that the third cluster is an equally representative mix of the above two clusters. This would fit the role of a working mom whose attention is split usually between her own needs and that of her family.


**Media person:**  In the fourth cluster, travel, computers ,politics , automotive have high importance. Fashion, beauty, cooking and photo-sharing also make the list. Thus, this cluster could represent a person who is mostly in the limelight and would like to stay abreast on global trends. They probably travel a lot for their work and hence are on the constant look out for new places and food to explore.


**Gen Z young-gun:** In cluster five, we see a high importance for automotive, current events, tv_film. We also see significant weights for fashion, beauty and photo-sharing. This would appear to be the subset of people who are young and are constantly on social media. They need to know what's happening in the world around and make their presence felt on the internet.


**College students:**  In cluster six, we see a high impportance for online_gaming, sports_playing, college_uni, school, sports_fandom, cooking and automotive. These are mostly viable topics of interest for students in College. Hence, we believe that this cluster would be an ideal match for College students.








